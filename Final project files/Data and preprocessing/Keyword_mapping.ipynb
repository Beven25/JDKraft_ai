{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_CXPigZqtu_"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "tokenizer_knowledge = AutoTokenizer.from_pretrained(\"jjzha/jobbert_knowledge_extraction\")\n",
        "model_knowledge = AutoModelForTokenClassification.from_pretrained(\"jjzha/jobbert_knowledge_extraction\")\n",
        "\n",
        "tokenizer_skill = AutoTokenizer.from_pretrained(\"jjzha/jobbert_skill_extraction\")\n",
        "model_skill = AutoModelForTokenClassification.from_pretrained(\"jjzha/jobbert_skill_extraction\")\n",
        "\n",
        "training_dataset = load_dataset(\"Appz7/jdgen\", split=\"train[:1000]\")"
      ],
      "metadata": {
        "id": "Ya71IWrWqvGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "token_skill_classifier = pipeline(model=\"jjzha/jobbert_skill_extraction\", aggregation_strategy=\"first\")\n",
        "token_knowledge_classifier = pipeline(model=\"jjzha/jobbert_knowledge_extraction\", aggregation_strategy=\"first\")\n"
      ],
      "metadata": {
        "id": "MiTid4inq24r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def aggregate_span(results):\n",
        "    new_results = []\n",
        "    current_result = results[0]\n",
        "\n",
        "    for result in results[1:]:\n",
        "        if result[\"start\"] == current_result[\"end\"] + 1:\n",
        "            current_result[\"word\"] += \" \" + result[\"word\"]\n",
        "            current_result[\"end\"] = result[\"end\"]\n",
        "        else:\n",
        "            new_results.append(current_result)\n",
        "            current_result = result\n",
        "\n",
        "    new_results.append(current_result)\n",
        "    return new_results\n",
        "\n",
        "def ner(text):\n",
        "    input_ids = token_skill_classifier.tokenizer.encode(text, truncation=True, add_special_tokens=False)\n",
        "    max_length = 510\n",
        "    input_chunks = [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
        "\n",
        "    all_skills = set()\n",
        "    all_knowledge = set()\n",
        "\n",
        "    for chunk_ids in input_chunks:\n",
        "        chunk_text = token_skill_classifier.tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
        "        output_skills = token_skill_classifier(chunk_text)\n",
        "        output_knowledge = token_knowledge_classifier(chunk_text)\n",
        "\n",
        "        if len(output_skills) > 0:\n",
        "            output_skills = aggregate_span(output_skills)\n",
        "        if len(output_knowledge) > 0:\n",
        "            output_knowledge = aggregate_span(output_knowledge)\n",
        "\n",
        "        chunk_skills = {res[\"word\"] for res in output_skills}\n",
        "        chunk_knowledge = {res[\"word\"] for res in output_knowledge}\n",
        "\n",
        "        all_skills.update(chunk_skills)\n",
        "        all_knowledge.update(chunk_knowledge)\n",
        "\n",
        "    final_knowledge = all_knowledge.difference(all_skills)\n",
        "\n",
        "    return all_skills.union(final_knowledge)\n",
        "\n",
        "def process_training_dataset(training_dataset):\n",
        "    training_data = []\n",
        "    for t_row in training_dataset:\n",
        "        cleaned_skills = ner(t_row['skills'])\n",
        "        cleaned_gpt_response = ner(t_row['gpt_response'])\n",
        "        training_data.append({\n",
        "            \"Skills\": t_row['skills'],\n",
        "            \"GPT Response\": t_row['gpt_response'],\n",
        "            \"Cleaned Skills\": \", \".join(sorted(cleaned_skills)),\n",
        "            \"Cleaned GPT Response Skills\": \", \".join(sorted(cleaned_gpt_response)),\n",
        "        })\n",
        "    training_df = pd.DataFrame(training_data)\n",
        "    return training_df\n",
        "\n",
        "\n",
        "training_df = process_training_dataset(training_dataset)\n",
        "\n",
        "def calculate_percentage_and_matched_keywords(df):\n",
        "    percentages = []\n",
        "    matched_keywords_list = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        cleaned_skills_set = set(row['Cleaned Skills'].split(\", \"))\n",
        "        cleaned_gpt_response_set = set(row['Cleaned GPT Response Skills'].split(\", \"))\n",
        "        matched_keywords = cleaned_skills_set.intersection(cleaned_gpt_response_set)\n",
        "        matched_keywords_list.append(\", \".join(sorted(matched_keywords)))\n",
        "\n",
        "        if cleaned_skills_set:\n",
        "            percentage = (len(matched_keywords) / len(cleaned_skills_set)) * 100\n",
        "        else:\n",
        "            percentage = 0\n",
        "\n",
        "        percentages.append(percentage)\n",
        "    df['Percentage Match'] = percentages\n",
        "    df['Matched Keywords'] = matched_keywords_list\n",
        "\n",
        "calculate_percentage_and_matched_keywords(training_df)"
      ],
      "metadata": {
        "id": "MtSY8VJo-VPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}